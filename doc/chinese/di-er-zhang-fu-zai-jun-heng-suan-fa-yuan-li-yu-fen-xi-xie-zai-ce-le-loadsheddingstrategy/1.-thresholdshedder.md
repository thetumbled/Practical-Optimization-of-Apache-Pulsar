# 1. ThresholdShedder

ThresholdShedder是默认的Shedding算法，我们优先分析它。

## **1.1 最大资源使用率**

ThresholdShedder 首先使用如下公式计算出每个Broker的最大资源使用率，资源包括CPU、直接内存、网卡入带宽、网卡出带宽。

`org.apache.pulsar.policies.data.loadbalancer.LocalBrokerData#getMaxResourceUsageWithWeight(double, double, double, double)`

<figure><img src="../.gitbook/assets/image (16).png" alt=""><figcaption></figcaption></figure>

资源使用率的计算还可以配置一个权值，由配置loadBalancerBandwidthInResourceWeight、loadBalancerBandwidthOutResourceWeight、loadBalancerCPUResourceWeight、loadBalancerDirectMemoryResourceWeight决定权值大小。

<figure><img src="../.gitbook/assets/image (17).png" alt=""><figcaption></figcaption></figure>

在 2.11 版本之前，负载均衡算法会考虑内存资源的使用率情况。然而，在实践过程中发现，broker 的负载与内存使用率并无明显关联，因此下面PR去掉了它：

[https://github.com/apache/pulsar/pull/19559](https://github.com/apache/pulsar/pull/19559)

实际上，直接内存使用率跟broker的负载也没有明显关联性，因此我提交了如下PR把loadBalancerDirectMemoryResourceWeight的默认值修改为0。

[https://github.com/apache/pulsar/pull/21168](https://github.com/apache/pulsar/pull/21168)

事实上，在负载均衡算法中，所考量的监控指标并非种类越多越好。过多的指标种类，反而容易引发各类意料之外的状况，排查时也会增添诸多麻烦。目前我们只考虑CPU、网卡带宽，这就足够了。



## **1.2 历史权值算法**

计算出来每个Broker的最大资源使用率后，还会执行一个历史权值算法来得到最终的打分。

`org.apache.pulsar.broker.loadbalance.impl.ThresholdShedder#updateAvgResourceUsage`

<figure><img src="../.gitbook/assets/image (18).png" alt=""><figcaption></figcaption></figure>

```
historyUsage = historyUsage == null ? resourceUsage : historyUsage * historyPercentage + (1 - historyPercentage) * resourceUsage;
brokerAvgResourceUsage.put(broker, historyUsage);
```

historyPercentage由配置loadBalancerHistoryResourcePercentage决定，默认值是0.9，即上一次计算得到的分数占90%，当前计算得到分数只占10%。

<figure><img src="../.gitbook/assets/image (19).png" alt=""><figcaption></figcaption></figure>

比如说，上一轮负载均衡执行时，broker的分数是80，而当前broker的资源使用率是50%，那么当前broker的负载分数为80\*0.9+50\*0.1=77，可以看到真实负载50和打分77之间存在较大的差距。

引入这个历史权值算法是为了避免短时间的异常负载抖动导致bundle切换，但这个算法会引入一个严重的问题：它会使得机器的真实负载和负载打分数值有较大的差距，从而误判低载机器为高载机器，高载为低载机器，导致错误的负载决策，即第一章所说的正确性问题，这一点会在下一章的LeastResourceUsageWithWeight小节详细阐述。



接下来计算出来整个集群所有broker的平均分数：avgUsage = totalUsage / totalBrokers，当任一broker的分数超过avgUsage一定阈值时，就判定该broker超载。阈值由配置loadBalancerBrokerThresholdShedderPercentage决定，默认值为10。

<figure><img src="../.gitbook/assets/image (20).png" alt=""><figcaption></figcaption></figure>

但是一个broker被判定为超载后，不一定会从它身上卸载流量。比如说，如果该broker只服务了一个bundle，一旦卸载此 bundle，会致使该 broker 不再承载任何流量，因此会直接跳过bundle unload。

因此，实践过程中我们要避免超大bundle的出现.例如，若单个 bundle 致使某一 broker 超载，那么将其放置于其他任何 broker 上，大概率也会引发超载现象。为有效规避超大 bundle，一方面要合理配置 bundle 的数量，另一方面要防止超大分区的出现。通常，当一个分区的流量吞吐达到 20MB/s 时，我们便会主动建议用户执行扩分区操作。当然，利用 bundle split 特性同样能够避免超大 bundle，但考虑到在业务高峰期触发 bundle split 可能导致业务流量抖动，我们并未开启该功能。关于 bundle 的更多内容，将在后续章节展开详细介绍。



## **1.3 挑选bundle**

那如果要卸载某个broker的流量，会如何挑选bundle进行unload呢？

首先计算出来要卸载多少流量，然后卸载至少达到该流量总量的 bundle。此处所提及的流量涵盖入流量与出流量。例如，若一个 bundle 的入流量为 10MB，出流量为 20MB，则其流量计算为 10 + 20 = 30MB；若一个 broker 的入流量为 1GB，出流量为 2GB，则其流量计算为 1 + 2 = 3GB 。

算法如下：

<figure><img src="../.gitbook/assets/image (21).png" alt=""><figcaption></figcaption></figure>

<figure><img src="../.gitbook/assets/image (22).png" alt=""><figcaption></figcaption></figure>

首先计算得到要卸载的流量占该broker总流量的比例

`percentOfTrafficToOffload = currentUsage - avgUsage - threshold + ADDITIONAL_THRESHOLD_PERCENT_MARGIN;`

* currentUsage为当前broker的负载分数（即经过历史权值算法计算后的结果）
* avgUsage为前面得到的平均broker分数
* threshold即前面介绍的配置loadBalancerBrokerThresholdShedderPercentage
* ADDITIONAL\_THRESHOLD\_PERCENT\_MARGIN为一个常量0.05

那么需要卸载的流量大小为

`minimumThroughputToOffload = brokerCurrentThroughput * percentOfTrafficToOffload;`

&#x20;

举个例子，假设一个broker的最大资源使用率为80%，avgUsage为60%，threshold为10，该broker的流量吞吐为10GB，则需要卸载(0.8-0.6-0.1+0.05)\*10=1.5GB对应的bundle。

&#x20;

另外，这里还有一个配置loadBalancerBundleUnloadMinThroughputThreshold，用来指定最小的卸载流量阈值，默认值为10MB，这是因为卸载太少流量对负载均衡没有明显效果，反而造成用户链接中断。

<figure><img src="../.gitbook/assets/image (23).png" alt=""><figcaption></figcaption></figure>

计算出来要卸载多少流量的bundle之后，就要开始挑选bundle了，挑选原则很简单，从大到小开始挑选，因此小流量吞吐的bundle几乎不可能在负载均衡过程中被挑选到。

&#x20;

